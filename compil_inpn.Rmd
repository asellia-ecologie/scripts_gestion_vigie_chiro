```{r}
library("tidyverse")
library("RPostgreSQL")
library("sf")
library("writexl")
library("readxl")
library("xlsx")
library(data.table)
library(sjmisc)
```

```{r}# Se mettre dans BDD/SONS depuis son ordi :
dossier <- "/home/bbk9/kDrive/Shared/BDD/SONS"
setwd(dossier)
date <- as.character(Sys.time())
date <- strsplit(date, " ")
date <- date[[1]][1]
# indication du dossier où trouver les sons a traiter
d <- "ANALYSES/csv"
chemin_csv <- file.path(dossier, d)
# et les scripts (pour faire appel à d’autres)
dos_scripts <- "00_SCRIPTS"
# Création d’une liste des fichiers csv
fichiers_csv <- list.files(chemin_csv,
                           pattern = "*.csv", full.names = TRUE)

# Choix des colonnes que l’on veut garder des compils tadarida :
colonnes <- c("Nom", "nom_du_fichier", "Type", "temps_debut", "temps_fin",
              "frequence_mediane", "tadarida_taxon", "tadarida_probabilite",
              "observateur_taxon",
              "observateur_probabilite", "validateur_taxon",
              "validateur_probabilite")

# ouverture des csv et chargement dans la liste 'tables' :
tables_compils <- lapply(fichiers_csv, fread, sep = ";", select = colonnes)

col <- colonnes[-2]
tables_compils <- lapply(tables_compils, setNames, col)
# mise bout à bout de toutes les tables
sons <- do.call(rbind, tables_compils)
sons <- rbind(sons, sons_a_faire)

# Si on veut enregistrer la compil dans un gros csv :
write.csv2(sons, paste0("compils_tadarida_", date, ".csv"))

# Ne garder que les lignes analysées :
sons_asellia <- sons %>% filter(!is.na(observateur_taxon))

rm(sons)
# Ne prendre que les sons supérieurs à 0.5 de proba :
sons_asellia$tadarida_probabilite <- str_replace(
                                        sons_asellia$tadarida_probabilite, ',', '.')
sons_asellia$tadarida_probabilite <- as.numeric(
                                        sons_asellia$tadarida_probabilite)
sons_asellia$simi <- ifelse(
                            sons_asellia$observateur_taxon ==
                              sons_asellia$tadarida_taxon,
                            TRUE, FALSE)


# Correction de observateur_taxon sous la forme
# Majmin
sons_asellia$observateur_taxon <- str_to_title( # met en Maj
                                    tolower( # ce qui a été mis en min
                                        sons_asellia$observateur_taxon
                                        ))


# Régler pb de certains sons SW5
sons_asellia$Nom <- str_replace(sons_asellia$Nom, "_00000_", "_")

# Création des colonnes issues du nom de fichier :
sons_asellia$nom_point <- str_split_i(sons_asellia$Nom, "_", 2)

sons_asellia$lieu_dit <- str_split_i(sons_asellia$Nom, "_", 3)

sons_asellia$boitier <- str_split_i(
                            str_split_i(
                                sons_asellia$Nom, "_", 1), "-", 5)

sons_asellia$date_heure <- ymd_hms(
    paste(str_split_i(sons_asellia$Nom, "_", -3),
          str_split_i(sons_asellia$Nom, "_", -2))
)
# unique(sons_asellia$nom_point)
unique(sons_asellia$lieu_dit)
unique(sons_asellia$date_heure)

sons_asellia %>% filter(is.na(date_heure))
sons_asellia <- sons_asellia %>% filter(!is.na(date_heure))
# Ajout du champ heure_stats pour représenter les activités horaires :
# Attention ! Tout est à la même date ne pas utiliser ailleurs.
sons_asellia$heure_stats <- ymd_hms(
                                paste("20000101",
                                   str_split_i(sons_asellia$Nom, "_", -2)
                                   ))


# Remplissage conditionnel du champ date_nuit :
sons_asellia$date_nuit <- dplyr::if_else(
        between(hour(sons_asellia$date_heure), 12, 23), # si entre midi-minuit
            format(date(sons_asellia$date_heure),       # on garde la date
                   format = "%Y-%m-%d"),
            format(date(sons_asellia$date_heure - days(1)), # sinon on prend
                   format = "%Y-%m-%d"))                    # la veille

# Deux manières de calculer les fréquences de similitudes
# (tadarda/observateur) par taxon :
sons_asellia %>% group_by(observateur_taxon) %>% frq(simi)
flat_table(sons_asellia, observateur_taxon, simi)

# La ligne suivante va chercher les identifiants
# pour se connecter à la bdd_placettes_2023 :
source(file.path(dossier, dos_scripts, "cred.R"))

# Puis on établit la connexion :
tryCatch({
    drv <- dbDriver("PostgreSQL")
    print("Connecting to Database...")
    connec <- dbConnect(drv,
                    dbname = dbname,
                    host = host,
                    port = port,
                    user = user,
                    password = password)
print("Database Connected!")
},
error = function(cond) {
    print("Unable to connect to Database.")
})

# Requete pour récupérer la bdd_placettes_2023 en entier :
query_totale <- paste("select geom, nombre_de_nuits,date, nom_point, obs1,
                      \"lieu-dit\", type_habitat",
                      "from bd_sons.bdd_placettes_2023")

# Exécution de la requete et stockage dans un tableau géolocalisé :
placettes <- st_read(connec,
                     query = query_totale, quiet = TRUE)

# Pour gérer l’intervale dans la jointure :
sons_asellia$date_nuit <- as.Date(sons_asellia$date_nuit)
placettes$nombre_de_nuits <- as.integer(placettes$nombre_de_nuits)

# Mise à 1 si le nombre de nuits n’a pas été renseigné :
placettes$nombre_de_nuits[is.na(placettes$nombre_de_nuits)] <- 1

# calcul de la dernière nuit de pose du boitier si plusieurs nuits :
placettes$date_max <- placettes$date + days(placettes$nombre_de_nuits - 1)

# création de la jointure entre le nom_point et l’intervale de date :
by <- join_by(nom_point,
              between(x$date_nuit, y$date, y$date_max))
by2 <- join_by(nom_point, x$lieu_dit == y$"lieu-dit")

# Nettoyage (à éviter) si des date_nuit manquent la jointure sera impossible :
sons_asellia <- sons_asellia %>% filter(!is.na(date_nuit))

sons_asellia <- unique(sons_asellia)
# jointure entre les sons et la bdd placettes
data_loc <- left_join(sons_asellia, placettes,
                      by2, multiple = "first")

perdus <- data_loc %>% filter(is.na(id))
unique(perdus$nom_point)
perdus %>% group_by(nom_point, date_nuit)
unique(paste(perdus$nom_point,  perdus$date_nuit))
class(data_loc)
data_loc <- st_as_sf(data_loc)
data_inpn <- data_loc %>% group_by(nom_point, date_nuit, observateur_taxon,
                                   obs1, type_habitat) %>%
  summarise(nombre = n())

data_inpn <- st_as_sf(data_inpn)
data_inpn <- data_inpn %>%  st_cast("POINT")
data_inpn$x_wgs84 <- st_coordinates(test)[, 1]
data_inpn$y_wgs84 <- st_coordinates(test)[, 2]

write.csv2(data_inpn, "export_inpn.csv")

```

